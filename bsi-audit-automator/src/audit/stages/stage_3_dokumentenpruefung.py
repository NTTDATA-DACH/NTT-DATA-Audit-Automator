# file: src/audit/stages/stage_3_dokumentenpruefung.py
import logging
import json
import asyncio
from typing import Dict, Any, List, Tuple
from datetime import datetime, timedelta
from google.cloud.exceptions import NotFound
from collections import defaultdict

from src.config import AppConfig
from src.clients.gcs_client import GcsClient
from src.clients.ai_client import AiClient
from src.clients.rag_client import RagClient
from src.audit.stages.control_catalog import ControlCatalog

class Chapter3Runner:
    """
    Handles generating content for Chapter 3 "Dokumentenprüfung" by dynamically
    parsing the master report template and using the central prompt configuration.
    It now relies on the pre-computed `extracted_grundschutz_check_merged.json`
    file generated by the `Grundschutz-Check-Extraction` stage.
    """
    STAGE_NAME = "Chapter-3"
    TEMPLATE_PATH = "assets/json/master_report_template.json"
    PROMPT_CONFIG_PATH = "assets/json/prompt_config.json"
    INTERMEDIATE_CHECK_RESULTS_PATH = "output/results/intermediate/extracted_grundschutz_check_merged.json"
    GROUND_TRUTH_MAP_PATH = "output/results/intermediate/system_structure_map.json"
    
    def __init__(self, config: AppConfig, gcs_client: GcsClient, ai_client: AiClient, rag_client: RagClient):
        self.config = config
        self.gcs_client = gcs_client
        self.ai_client = ai_client
        self.rag_client = rag_client
        self.control_catalog = ControlCatalog()
        self.prompt_config = self._load_asset_json(self.PROMPT_CONFIG_PATH)
        self.execution_plan = self._build_execution_plan_from_template()
        self._doc_map = self.rag_client._document_category_map
        self._ground_truth_map = None # Lazy loaded
        logging.info(f"Initialized runner for stage: {self.STAGE_NAME} with dynamic execution plan.")

    def _load_asset_json(self, path: str) -> dict:
        with open(path, 'r', encoding='utf-8') as f: return json.load(f)

    async def _get_ground_truth_map(self) -> Dict[str, Any]:
        """Lazy loads the ground truth map and caches it."""
        if self._ground_truth_map is None:
            try:
                self._ground_truth_map = await self.gcs_client.read_json_async(self.GROUND_TRUTH_MAP_PATH)
            except NotFound:
                logging.error(f"FATAL: Ground truth map not found at '{self.GROUND_TRUTH_MAP_PATH}'. Please run the extraction stage.")
                raise
        return self._ground_truth_map

    async def _process_details_zum_it_grundschutz_check(self) -> Dict[str, Any]:
        """
        Uses the pre-computed/refined data to answer the five questions with a mix of
        deterministic and targeted AI-driven logic.
        """
        logging.info("Processing 3.6.1 'Details zum IT-Grundschutz-Check' using pre-computed data...")
        try:
            check_data = self.gcs_client.read_json(self.INTERMEDIATE_CHECK_RESULTS_PATH)
            anforderungen = check_data.get("anforderungen", [])
        except NotFound:
            logging.error(f"FATAL: The required intermediate file '{self.INTERMEDIATE_CHECK_RESULTS_PATH}' was not found. Please run the 'Grundschutz-Check-Extraction' stage first.")
            raise

        answers = [None] * 5
        findings = []
        ground_truth_map = await self._get_ground_truth_map()

        # Task E: Coverage Check
        all_mapped_kuerzel = {k for k_list in ground_truth_map.get("baustein_to_zielobjekt_mapping", {}).values() for k in k_list}
        all_checked_kuerzel = {a.get("zielobjekt_kuerzel") for a in anforderungen}
        missing_in_check = all_mapped_kuerzel - all_checked_kuerzel
        if missing_in_check:
            desc = f"Die Zielobjekte {sorted(list(missing_in_check))} sind in der Modellierung vorhanden, aber es wurden für sie keine Anforderungen im Grundschutz-Check gefunden oder verarbeitet."
            findings.append({"category": "AG", "description": desc})
            logging.warning(f"Coverage Check (Task E) failed: {desc}")

        # Q1: Status erhoben? (Deterministic)
        answers[0] = all(a.get("umsetzungsstatus") for a in anforderungen)
        if not answers[0]:
            findings.append({"category": "AG", "description": "Nicht für alle Anforderungen wurde ein Umsetzungsstatus erhoben."})

        # Q5: Prüfung < 12 Monate? (Deterministic)
        one_year_ago = datetime.now() - timedelta(days=365)
        outdated = [a for a in anforderungen if datetime.strptime(a.get("datumLetztePruefung", "1970-01-01"), "%Y-%m-%d") < one_year_ago]
        answers[4] = not bool(outdated)
        if outdated:
            findings.append({"category": "AG", "description": f"Die Prüfung von {len(outdated)} Anforderungen liegt mehr als 12 Monate zurück."})
            
        targeted_prompt_config = self.prompt_config["stages"]["Chapter-3"]["targeted_question"]
        targeted_prompt_template = targeted_prompt_config["prompt"]

        # Q2: "entbehrlich" plausibel? (Targeted AI - Task D)
        entbehrlich_items = [a for a in anforderungen if a.get("umsetzungsstatus") == "entbehrlich"]
        risikoanalyse_uris = self.rag_client.get_gcs_uris_for_categories(["Risikoanalyse"])
        if entbehrlich_items:
            for item in entbehrlich_items: # Enrich with control level
                item['level'] = self.control_catalog.get_control_level(item.get('id'))
            
            question_text = "Sind die Begründungen für 'entbehrlich' plausibel? BSI-Regel: Eine Anforderung mit Level 5 ist immer entbehrlich, außer  wenn sie durch in der beigefügten Risikoanalyse explizit gefordert wird."
            prompt = targeted_prompt_template.format(
                question=question_text,
                json_data=json.dumps(entbehrlich_items, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), gcs_uris=risikoanalyse_uris, request_context_log="3.6.1-Q2")
            answers[1], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[1] = True

        # Q3: MUSS-Anforderungen erfüllt? (Targeted AI)
        level_1_ids = self.control_catalog.get_level_1_control_ids()
        muss_anforderungen = [a for a in anforderungen if a.get("id") in level_1_ids]
        if muss_anforderungen:
            prompt = targeted_prompt_template.format(
                question="Sind alle diese MUSS-Anforderungen (Level 1) mit Status 'Ja' umgesetzt?",
                json_data=json.dumps(muss_anforderungen, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), request_context_log="3.6.1-Q3")
            answers[2], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[2] = True

        # Q4: Nicht/teilweise umgesetzte in A.6? (Targeted AI)
        unmet_items = [a for a in anforderungen if a.get("umsetzungsstatus") in ["Nein", "teilweise"]]
        realisierungsplan_uris = self.rag_client.get_gcs_uris_for_categories(["Realisierungsplan"])
        if unmet_items and realisierungsplan_uris:
            prompt = targeted_prompt_template.format(
                question="Sind diese nicht oder teilweise umgesetzten Anforderungen im angehängten Realisierungsplan (A.6) dokumentiert?",
                json_data=json.dumps(unmet_items, indent=2, ensure_ascii=False)
            )
            res = await self.ai_client.generate_json_response(prompt, self._load_asset_json("assets/schemas/generic_1_question_schema.json"), gcs_uris=realisierungsplan_uris, request_context_log="3.6.1-Q4")
            answers[3], findings = (res['answers'][0], findings + [res['finding']] if res['finding']['category'] != 'OK' else findings)
        else:
            answers[3] = not unmet_items
            if unmet_items and not realisierungsplan_uris:
                findings.append({"category": "AG", "description": "Es gibt nicht umgesetzte Anforderungen, aber der Realisierungsplan (A.6) wurde nicht gefunden, um die Dokumentation zu überprüfen."})

        # Consolidate findings
        final_finding = {"category": "OK", "description": "Alle Prüfungen für den IT-Grundschutz-Check waren erfolgreich."}
        if findings:
            final_finding["category"] = "AS" if any(f['category'] == 'AS' for f in findings) else "AG"
            final_finding["description"] = "Zusammenfassung: " + " | ".join([f['description'] for f in findings])

        return {"detailsZumItGrundschutzCheck": {"answers": answers, "finding": final_finding}}

    def _check_document_coverage(self) -> Dict[str, Any]:
        """Checks if all critical BSI document types are present."""
        REQUIRED_CATEGORIES = {
            "Sicherheitsleitlinie", "Strukturanalyse", "Schutzbedarfsfeststellung",
            "Modellierung", "Grundschutz-Check", "Risikoanalyse", "Realisierungsplan"
        }
        present_categories = set(self._doc_map.keys())
        missing_categories = REQUIRED_CATEGORIES - present_categories

        if not missing_categories:
            return {"category": "OK", "description": "Alle kritischen Dokumententypen sind vorhanden."}
        else:
            desc = f"Kritische Dokumente fehlen: {', '.join(sorted(list(missing_categories)))}. Dies ist eine schwerwiegende Abweichung."
            logging.warning(f"Document coverage check failed. Missing: {missing_categories}")
            return {"category": "AS", "description": desc}

    def _build_execution_plan_from_template(self) -> List[Dict[str, Any]]:
        """Parses master_report_template.json to build a dynamic list of tasks."""
        plan = []
        template = self._load_asset_json(self.TEMPLATE_PATH)
        ch3_template = template.get("bsiAuditReport", {}).get("dokumentenpruefung", {})
        
        for subchapter_name, subchapter_data in ch3_template.items():
             if not isinstance(subchapter_data, dict): continue
             task = self._create_task_from_section(subchapter_name, subchapter_data)
             if task: plan.append(task)
             for section_key, section_data in subchapter_data.items():
                if isinstance(section_data, dict):
                    task = self._create_task_from_section(section_key, section_data)
                    if task: plan.append(task)
        return plan

    def _create_task_from_section(self, key: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Creates a single task dictionary for the execution plan."""
        task_config = self.prompt_config["stages"]["Chapter-3"].get(key)
        if not task_config: return None

        task = {"key": key, "type": task_config.get("type", "ai_driven")}
        if task["type"] == "custom_logic": return task

        task["schema_path"] = task_config["schema_path"]
        task["source_categories"] = task_config.get("source_categories")

        if task["type"] == "ai_driven" or key == 'modellierungsdetails':
            generic_prompt = self.prompt_config["stages"]["Chapter-3"]["generic_question"]["prompt"]
            # For modellierungsdetails, the prompt is custom in the config
            task['prompt'] = task_config.get('prompt', generic_prompt)
            questions = [item["questionText"] for item in data.get("content", []) if item.get("type") == "question"]
            task["questions_formatted"] = "\n".join(f"{i+1}. {q}" for i, q in enumerate(questions))
        elif task["type"] == "summary":
            task["prompt"] = self.prompt_config["stages"]["Chapter-3"]["generic_summary"]["prompt"]
            task["summary_topic"] = data.get("title", key)
        return task

    async def _process_ai_subchapter(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Generates content for a single AI-driven subchapter."""
        key, schema_path = task["key"], task["schema_path"]
        
        prompt_format_args = {"questions": task.get("questions_formatted", "")}

        # Task C: Inject ground truth context for modellierungsdetails
        if key == 'modellierungsdetails':
            ground_truth_map = await self._get_ground_truth_map()
            zielobjekte_list = ground_truth_map.get('zielobjekte', [])
            prompt_format_args['zielobjekte_json'] = json.dumps(zielobjekte_list, indent=2, ensure_ascii=False)
        
        prompt = task["prompt"].format(**prompt_format_args)
        uris = self.rag_client.get_gcs_uris_for_categories(task.get("source_categories"))
        
        if not uris and task.get("source_categories") is not None:
             return {key: {"error": f"No source documents for categories: {task.get('source_categories')}"}}
        try:
            data = await self.ai_client.generate_json_response(prompt, self._load_asset_json(schema_path), uris, f"Chapter-3: {key}")
            if key == "aktualitaetDerReferenzdokumente":
                coverage_finding = self._check_document_coverage()
                if coverage_finding['category'] != 'OK': data['finding'] = coverage_finding
            return {key: data}
        except Exception as e:
            logging.error(f"Failed to generate for {key}: {e}", exc_info=True)
            return {key: {"error": str(e)}}

    async def _process_summary_subchapter(self, task: Dict[str, Any], previous_findings: str) -> Dict[str, Any]:
        """Generates a summary/verdict for a subchapter."""
        key = task["key"]
        prompt = task["prompt"].format(summary_topic=task["summary_topic"], previous_findings=previous_findings)
        try:
            return {key: await self.ai_client.generate_json_response(prompt, self._load_asset_json(task["schema_path"]), request_context_log=f"Chapter-3 Summary: {key}")}
        except Exception as e:
            return {key: {"error": str(e)}}

    def _get_findings_from_results(self, results_list: List[Dict]) -> str:
        """Extracts and formats findings from a list of results for summary prompts."""
        findings = []
        for res_dict in results_list:
            if not res_dict: continue
            result_data = list(res_dict.values())[0]
            if isinstance(result_data, dict) and isinstance(result_data.get('finding'), dict):
                finding = result_data['finding']
                if finding.get('category') != "OK":
                    findings.append(f"- [{finding.get('category')}]: {finding.get('description')}")
        return "\n".join(findings) if findings else "No specific findings were generated."

    async def run(self, force_overwrite: bool = False) -> dict:
        """Executes the dynamically generated plan for Chapter 3."""
        logging.info(f"Executing dynamically generated plan for stage: {self.STAGE_NAME}")
        
        aggregated_results, processed_results = {}, []
        custom_tasks = [t for t in self.execution_plan if t and t.get("type") == "custom_logic"]
        ai_tasks = [t for t in self.execution_plan if t and (t.get("type") == "ai_driven" or t.get("key") == "modellierungsdetails")]
        summary_tasks = [t for t in self.execution_plan if t and t.get("type") == "summary"]

        for task in custom_tasks:
            key = task['key']
            logging.info(f"--- Processing custom logic task: {key} ---")
            if key == 'detailsZumItGrundschutzCheck':
                result = await self._process_details_zum_it_grundschutz_check()
                processed_results.append(result)
                aggregated_results.update(result)
        
        if ai_tasks:
            ai_coroutines = [self._process_ai_subchapter(task) for task in ai_tasks]
            ai_results = await asyncio.gather(*ai_coroutines)
            processed_results.extend(ai_results)
            for res in ai_results: aggregated_results.update(res)

        if summary_tasks:
            findings_text = self._get_findings_from_results(processed_results)
            summary_coroutines = [self._process_summary_subchapter(task, findings_text) for task in summary_tasks]
            for res in await asyncio.gather(*summary_coroutines): aggregated_results.update(res)

        logging.info(f"Successfully aggregated results for all of stage {self.STAGE_NAME}")
        return aggregated_results